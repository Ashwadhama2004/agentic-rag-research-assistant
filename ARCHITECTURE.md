# Multi-Tenant Agentic RAG Research & Knowledge Assistant

## Project Architecture Documentation

Built around Endee Labs Endee as the semantic memory backbone

---

## Table of Contents

1. [Problem Statement](#problem-statement)
2. [High-Level Solution Overview](#high-level-solution-overview)
3. [Core Technology Stack](#core-technology-stack)
4. [System Architecture](#system-architecture)
5. [Endee Usage Design](#endee-usage-design)
6. [Component Breakdown](#component-breakdown)
7. [API Design](#api-design)
8. [UI Layer](#ui-layer)
9. [Deployment Strategy](#deployment-strategy)
10. [Performance Metrics & Monitoring](#performance-metrics--monitoring)
11. [Project Structure](#project-structure)
12. [Implementation Timeline](#implementation-timeline)
13. [Why This Architecture Wins](#why-this-architecture-wins)

---

## Problem Statement

### The Problem

Modern teams and individuals deal with:

- Large volumes of technical documents
- Rapidly changing external research
- Repeated questions with lost context
- Fragmented tools (search, notes, chat, docs)

### Why Traditional Solutions Fail

- Keyword search misses semantic meaning
- Chatbots hallucinate and lack memory
- Research tools don't integrate private documents
- Systems don't scale across multiple users securely

### Our Goal

Design and implement a production-ready AI system that:

- Answers questions grounded in user-uploaded documents
- Performs multi-step autonomous research
- Maintains long-term semantic memory
- Supports multiple users (multi-tenancy)
- Uses a vector database as the core system primitive
- Provides comprehensive performance metrics and monitoring
- Deploys seamlessly to Streamlit Cloud

---

## High-Level Solution Overview

### What This System Is

A multi-tenant agentic RAG platform that combines:

- **RAG (Retrieval-Augmented Generation)** for document-grounded answers
- **Agentic AI** for planning, reasoning, and multi-step research
- **Vector database-centric memory** using Endee
- **User-isolated namespaces** for secure multi-tenancy
- **Streamlit Web UI** for interaction and transparency
- **Real-time metrics** for performance tracking and optimization

---

## Core Technology Stack

### Backend

- Python 3.10+
- FastAPI (API layer)
- Pydantic (schemas)
- AsyncIO (concurrency)

### AI / ML

- Embedding model: SentenceTransformers (all-MiniLM-L6-v2 or all-mpnet-base-v2)
- LLM: Ollama with Llama 3.1 or Mistral (local/free) or Groq API (free tier)
- Agent framework (custom implementation with planning and reasoning)

### Vector Database

**Endee** - The core memory system

- GitHub: https://github.com/EndeeLabs/endee
- Purpose: Semantic memory, multi-tenant data isolation, fast retrieval

### Storage

- Endee (semantic data and embeddings)
- SQLite (metadata, users, sessions)

### Frontend

- **Streamlit** (primary UI framework)
- Streamlit components for interactive visualizations
- Plotly for dynamic charts and graphs
- Matplotlib/Seaborn for static metrics visualization

### Deployment

- **Streamlit Cloud** (primary deployment platform)
- GitHub integration for continuous deployment
- Environment-based configuration (.env)

### DevOps & Monitoring

- Docker (local development)
- Logging (Python logging module)
- Metrics tracking (custom implementation)
- Performance benchmarking suite

---

## LLM & Embedding Options (No OpenAI Required)

### Embedding Models (Free & Open Source)

This project uses **SentenceTransformers** for all embeddings - completely free and runs locally.

**Recommended Models:**

1. **all-MiniLM-L6-v2** (Default)
   - Size: 80MB
   - Speed: Very Fast
   - Dimensions: 384
   - Best for: Quick development, testing
   
2. **all-mpnet-base-v2** (Higher Quality)
   - Size: 420MB
   - Speed: Fast
   - Dimensions: 768
   - Best for: Production, better accuracy

**Installation:**
```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(["Your text here"])
```

### LLM Options (Choose One)

#### Option 1: Groq API (Recommended)

**Why Groq:**
- Free tier available (no credit card required)
- Extremely fast inference (fastest in the market)
- Multiple models available (Llama 3.1, Mixtral, Gemma)
- Simple API similar to OpenAI
- Great for development and production

**Setup:**
1. Sign up at https://console.groq.com
2. Get free API key
3. Add to secrets: `GROQ_API_KEY = "gsk_..."`

**Available Models:**
- `llama-3.1-70b-versatile` - Best for complex reasoning
- `llama-3.1-8b-instant` - Fastest responses
- `mixtral-8x7b-32768` - Long context window

**Usage:**
```python
from groq import Groq
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

response = client.chat.completions.create(
    model="llama-3.1-70b-versatile",
    messages=[{"role": "user", "content": "Your prompt"}]
)
```

**Limits (Free Tier):**
- 30 requests per minute
- 6,000 requests per day
- Sufficient for development and small deployments

#### Option 2: Ollama (Completely Local & Free)

**Why Ollama:**
- Completely free, no API keys
- Runs entirely on your machine
- Privacy-focused (no data sent externally)
- Good for development

**Setup:**
1. Install Ollama: https://ollama.ai
2. Download model: `ollama pull llama3.1`
3. Run: `ollama serve`

**Available Models:**
- `llama3.1` - Meta's latest (8B, 70B, 405B)
- `mistral` - Fast and efficient
- `phi3` - Small but capable (3.8B)

**Usage:**
```python
import ollama

response = ollama.chat(
    model='llama3.1',
    messages=[{'role': 'user', 'content': 'Your prompt'}]
)
```

**Requirements:**
- RAM: 8GB minimum (16GB recommended)
- Disk: 5-50GB depending on model
- GPU: Optional but recommended for speed

#### Option 3: HuggingFace Transformers (Advanced)

For complete control, run models directly via HuggingFace:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B")
```

**Note:** Requires more setup and computational resources.

### Recommended Setup for This Project

**For Development:**
- Embeddings: `all-MiniLM-L6-v2` (SentenceTransformers)
- LLM: **Groq API** with `llama-3.1-8b-instant`
- Reason: Fast, free, easy setup

**For Production:**
- Embeddings: `all-mpnet-base-v2` (SentenceTransformers)
- LLM: **Groq API** with `llama-3.1-70b-versatile`
- Reason: Better quality, still free tier

**For Complete Privacy:**
- Embeddings: `all-MiniLM-L6-v2` (local)
- LLM: **Ollama** with `llama3.1`
- Reason: No external API calls

### Cost Comparison

| Option | Embeddings Cost | LLM Cost | Total Monthly |
|--------|----------------|----------|---------------|
| OpenAI | $0.0001/1K tokens | $0.002/1K tokens | $50-200 |
| Groq API | $0 (local) | $0 (free tier) | **$0** |
| Ollama | $0 (local) | $0 (local) | **$0** |
| HuggingFace | $0 (local) | $0 (local) | **$0** |

**This project uses $0 in API costs.**

---

## System Architecture

### Conceptual Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      USER (Browser / Streamlit UI)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Streamlit Frontend Layer          â”‚
â”‚  - Document Upload Interface            â”‚
â”‚  - Chat Interface                       â”‚
â”‚  - Metrics Dashboard                    â”‚
â”‚  - Agent Visualization                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI Backend                 â”‚
â”‚  - Authentication & Authorization       â”‚
â”‚  - Request Routing                      â”‚
â”‚  - Session Management                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚
         v                   v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agentic Layer â”‚   â”‚   RAG Engine     â”‚
â”‚  (Planning &   â”‚   â”‚ (Document-based  â”‚
â”‚   Reasoning)   â”‚   â”‚      QA)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    v
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   ENDEE VECTOR DB    â”‚
         â”‚  Multi-tenant Core   â”‚
         â”‚   Semantic Memory    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    v
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  LLM (OpenAI API)    â”‚
         â”‚ Answer Generation    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture Principles

1. **Endee sits at the center** - Not as a bolt-on, but as the core data primitive
2. **Multi-tenancy by design** - User isolation from the ground up
3. **Agent-first approach** - Beyond simple Q&A to autonomous research
4. **Metrics-driven** - Every operation tracked and measured
5. **Production-ready** - Deployment, monitoring, and scalability baked in

---

## Endee Usage Design

### Why Endee?

Endee is used as the core semantic memory system because:

- Native support for vector embeddings
- Fast similarity search
- Lightweight and embeddable
- Multi-collection architecture perfect for multi-tenancy
- Open-source and actively maintained

### Collection Strategy

Each user (tenant) gets logically isolated collections:

| Collection Name | Purpose | Data Stored |
|----------------|---------|-------------|
| `user_{id}_docs` | Uploaded documents | Document chunks with embeddings |
| `user_{id}_research` | External research cache | Web search results, papers |
| `user_{id}_conversation` | Chat history | Conversation memory |
| `user_{id}_agent_steps` | Agent execution trace | Planning, reasoning steps |
| `user_{id}_summaries` | Long-term memory | Condensed knowledge |

### Multi-Tenancy Strategy

**Namespace prefix per user:**

```
user_123_docs
user_123_research
user_123_conversation
user_123_agent_steps
user_123_summaries

user_456_docs
user_456_research
...
```

**Benefits:**

- Complete data isolation
- No cross-user data leakage
- Scalable SaaS architecture
- Easy user deletion/cleanup
- Clear audit trail

---

## Component Breakdown

### A. Document Ingestion Pipeline

**Flow:**

1. User uploads PDF/text/markdown file via Streamlit interface
2. File is parsed using appropriate parser (PyPDF2, python-docx, etc.)
3. Text is chunked using recursive character splitter
   - Chunk size: 1000 characters
   - Chunk overlap: 200 characters
4. Each chunk is converted to embedding vector
5. Stored in Endee `user_{id}_docs` collection with metadata:
   - `document_name`
   - `page_number`
   - `chunk_index`
   - `user_id`
   - `upload_timestamp`

**Why This Matters:**

- Chunking affects retrieval accuracy
- Overlap preserves context across boundaries
- Embeddings enable semantic (not keyword) recall
- Metadata enables precise citation and tracing

**Performance Tracking:**

- Documents processed per second
- Average chunk size
- Embedding generation time
- Storage latency

### B. RAG Engine (Document QA Mode)

**Trigger:**

Simple factual question:
```
"What does our API policy say about rate limiting?"
```

**Execution Steps:**

1. Convert question â†’ embedding vector
2. Search Endee `user_{id}_docs` collection
3. Retrieve top-K relevant chunks (K=5 default)
4. Rerank chunks by relevance score
5. Construct prompt with chunks + question
6. Pass to LLM for answer generation
7. Extract answer with inline citations

**Output:**

```
According to the API documentation (page 12), rate limiting is 
set at 1000 requests per hour for standard accounts...

Sources:
- API_Documentation.pdf (page 12, chunk 45)
```

**Characteristics:**

- Deterministic retrieval
- Grounded in documents
- Fully explainable
- Fast (<2 seconds end-to-end)

**Performance Metrics:**

- Retrieval precision at K
- Answer relevance score
- Citation accuracy
- Response latency

### C. Agentic Research Engine (Advanced Mode)

**Trigger:**

Open-ended research request:
```
"Research recent advancements in RAG systems and compare 
them with our current implementation"
```

**Agent Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Agent Controller            â”‚
â”‚  - Task Planning                    â”‚
â”‚  - Step Execution                   â”‚
â”‚  - State Management                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Agent Loop                      â”‚
â”‚                                           â”‚
â”‚  1. THINK   - Classify task type          â”‚
â”‚  2. PLAN    - Decide execution steps      â”‚
â”‚  3. ACT     - Execute tools               â”‚
â”‚  4. REFLECT - Check completeness          â”‚
â”‚  5. RESPOND - Synthesize output           â”‚
â”‚                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Available Tools:**

- `vector_search`: Search Endee collections
- `web_search`: External research (if enabled)
- `document_upload`: Process new documents
- `summarize`: Create condensed summaries
- `compare`: Cross-reference internal/external data

**Agent Uses Endee To:**

- Recall previous research (avoid duplication)
- Store intermediate results
- Maintain long-term memory
- Cross-reference internal + external knowledge
- Track reasoning chain

**Example Execution Trace:**

```
Step 1: Planning
â””â”€â”€ Identified 3 sub-tasks:
    1. Search internal docs for current RAG implementation
    2. Research recent RAG papers (2024-2025)
    3. Compare and synthesize findings

Step 2: Internal Document Search
â””â”€â”€ Found 12 relevant chunks in user_docs
â””â”€â”€ Stored results in agent_steps collection

Step 3: External Research
â””â”€â”€ Searched for "RAG advancements 2024"
â””â”€â”€ Found 8 relevant papers
â””â”€â”€ Stored in research collection

Step 4: Comparison & Synthesis
â””â”€â”€ Cross-referenced findings
â””â”€â”€ Generated comparison table
â””â”€â”€ Created summary in summaries collection

Step 5: Response Generation
â””â”€â”€ Final report with citations
```

**Performance Metrics:**

- Agent task completion rate
- Average steps per task
- Tool usage distribution
- Planning accuracy
- Execution time per step

### D. Memory Management

**Short-Term Memory:**

- Recent conversation (last 10 messages)
- Stored in `user_{id}_conversation` collection
- Retrieved for context in each new query

**Long-Term Memory:**

- Summaries of past research
- Key insights and findings
- Stored as compressed embeddings in `user_{id}_summaries`
- Retrieved when semantically relevant

**Memory Consolidation:**

- Periodic summarization of old conversations
- Embedding-based compression
- Automatic cleanup of redundant data

**This Enables:**

```
User: "Continue where we left off last week"
System: [Retrieves summary from long-term memory]
        "Last week we discussed RAG optimization. You were 
        comparing chunking strategies. Should we continue 
        with the overlap analysis?"
```

**Performance Metrics:**

- Memory retrieval latency
- Compression ratio
- Context relevance score
- Memory hit rate

---

## API Design

### Core Endpoints

```
Authentication:
POST   /api/auth/register
POST   /api/auth/login
POST   /api/auth/logout
GET    /api/auth/session

Document Management:
POST   /api/documents/upload
GET    /api/documents/list
GET    /api/documents/{doc_id}
DELETE /api/documents/{doc_id}
GET    /api/documents/{doc_id}/chunks

Query & Chat:
POST   /api/chat/query
POST   /api/chat/conversation
GET    /api/chat/history

Agent Operations:
POST   /api/agent/research
GET    /api/agent/tasks
GET    /api/agent/tasks/{task_id}/status
GET    /api/agent/tasks/{task_id}/trace

Memory:
GET    /api/memory/conversations
GET    /api/memory/summaries
POST   /api/memory/consolidate

Metrics:
GET    /api/metrics/system
GET    /api/metrics/user
GET    /api/metrics/performance
```

### Request/Response Examples

**Document Upload:**

```json
POST /api/documents/upload

Request:
{
  "file": "<base64_encoded_content>",
  "filename": "research_paper.pdf",
  "metadata": {
    "tags": ["AI", "RAG"],
    "category": "research"
  }
}

Response:
{
  "document_id": "doc_789",
  "chunks_created": 45,
  "processing_time": 2.3,
  "status": "success"
}
```

**RAG Query:**

```json
POST /api/chat/query

Request:
{
  "query": "What is the recommended chunk size?",
  "mode": "rag",
  "top_k": 5
}

Response:
{
  "answer": "The recommended chunk size is 1000 characters...",
  "sources": [
    {
      "document": "RAG_Best_Practices.pdf",
      "page": 5,
      "chunk_id": "chunk_23",
      "relevance_score": 0.94
    }
  ],
  "latency": 1.2
}
```

**Agent Research:**

```json
POST /api/agent/research

Request:
{
  "task": "Research latest RAG improvements",
  "include_external": true,
  "max_steps": 10
}

Response:
{
  "task_id": "task_456",
  "status": "running",
  "estimated_time": 30,
  "steps_completed": 0
}
```

### All endpoints scoped by `user_id` from authenticated session

---

## UI Layer

### Streamlit Application Architecture

**Primary Interface:** Streamlit Web Application

**Key Pages/Sections:**

1. **Home Dashboard**
   - Quick stats overview
   - Recent activity
   - System status

2. **Document Management**
   - Drag-and-drop upload interface
   - Document list with metadata
   - Document preview
   - Delete/manage documents

3. **Chat Interface**
   - Message input area
   - Conversation history
   - Source citations display
   - Mode selector (RAG vs Agent)

4. **Agent Workspace**
   - Task creation interface
   - Real-time step visualization
   - Agent reasoning display
   - Task history

5. **Metrics Dashboard**
   - System performance charts
   - User statistics
   - Query analytics
   - Cost tracking

6. **Settings**
   - User preferences
   - API configuration
   - Model selection
   - Memory management

### UI Features

**Document Management Section:**

- Upload files (PDF, TXT, DOCX, MD)
- View indexed documents with metadata
- Search through documents
- Delete/archive documents
- Document processing status

**Chat Interface:**

- Natural language input
- Real-time streaming responses
- Inline source citations
- Follow-up question suggestions
- Conversation export

**Agent Visualization:**

Display agent execution in real-time:
```
Step 1: Planning                    [âœ“]
â””â”€â”€ Breaking task into 3 sub-tasks

Step 2: Internal Search             [âœ“]
â””â”€â”€ Found 12 relevant documents

Step 3: External Research           [âŸ³]
â””â”€â”€ Searching academic papers...
```

**Research Output:**

- Structured reports with sections
- Citation management
- Downloadable formats (Markdown, PDF)
- Share/export functionality

### Visualization Components

**Performance Metrics Charts:**

1. **Query Latency Distribution**
   - Histogram showing response times
   - P50, P95, P99 markers
   - Time series over last 24h/7d/30d

2. **Retrieval Accuracy Plot**
   - Precision@K curves
   - Recall metrics
   - F1 scores over time

3. **Token Usage Tracking**
   - Daily token consumption
   - Cost estimation
   - Breakdown by operation type

4. **Agent Performance**
   - Task completion rates
   - Average steps per task
   - Success/failure distribution

5. **Document Statistics**
   - Documents uploaded over time
   - Storage usage
   - Most queried documents

6. **User Engagement**
   - Active users timeline
   - Query volume heatmap
   - Feature usage distribution

### Streamlit-Specific Implementation

```python
# Example structure of main Streamlit app
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go

# Page configuration
st.set_page_config(
    page_title="Agentic RAG Assistant",
    page_icon="ğŸ§ ",
    layout="wide"
)

# Sidebar navigation
page = st.sidebar.selectbox(
    "Navigate",
    ["Dashboard", "Documents", "Chat", "Agent", "Metrics", "Settings"]
)

# Main content area based on selection
if page == "Dashboard":
    render_dashboard()
elif page == "Documents":
    render_document_manager()
elif page == "Chat":
    render_chat_interface()
elif page == "Agent":
    render_agent_workspace()
elif page == "Metrics":
    render_metrics_dashboard()
else:
    render_settings()
```

---

## Deployment Strategy

### Streamlit Cloud Deployment

**Platform:** Streamlit Community Cloud (https://streamlit.io/cloud)

**Deployment Process:**

1. **Repository Setup**
   - Host code on GitHub (public or private)
   - Include `requirements.txt`
   - Add `secrets.toml` configuration template
   - Include `.streamlit/config.toml` for app configuration

2. **Configuration Files**

   `requirements.txt`:
   ```
   streamlit==1.31.0
   fastapi==0.109.0
   uvicorn==0.27.0
   pydantic==2.5.0
   
   # LLM Options (choose one or both)
   groq==0.4.2                    # Groq API client (recommended - free tier available)
   ollama==0.1.6                  # Ollama local LLM client (completely free)
   
   # Embeddings
   sentence-transformers==2.3.1   # Open-source embeddings
   
   # Document processing
   pypdf2==3.0.1
   python-docx==1.1.0
   python-multipart==0.0.6
   
   # Data & ML
   numpy==1.26.3
   pandas==2.2.0
   torch==2.1.2                   # Required by sentence-transformers
   
   # Visualization
   plotly==5.18.0
   matplotlib==3.8.2
   seaborn==0.13.1
   
   # Database & Storage
   sqlalchemy==2.0.25
   endee==0.1.0                   # Adjust based on actual package
   
   # Utils
   python-jose[cryptography]==3.3.0  # JWT tokens
   passlib[bcrypt]==1.7.4            # Password hashing
   python-dotenv==1.0.0
   ```

   `.streamlit/config.toml`:
   ```toml
   [theme]
   primaryColor = "#FF4B4B"
   backgroundColor = "#FFFFFF"
   secondaryBackgroundColor = "#F0F2F6"
   textColor = "#262730"
   font = "sans serif"

   [server]
   maxUploadSize = 200
   enableXsrfProtection = true
   enableCORS = false
   ```

3. **Secrets Management**

   In Streamlit Cloud dashboard, configure secrets:
   ```toml
   # .streamlit/secrets.toml (not committed to repo)
   # Option 1: Using Groq (free tier, fast inference)
   GROQ_API_KEY = "gsk_..."  # Get from https://console.groq.com
   
   # Option 2: Using Ollama (completely local, no API key needed)
   OLLAMA_BASE_URL = "http://localhost:11434"
   
   DATABASE_URL = "sqlite:///./app.db"
   SECRET_KEY = "your-secret-key-here"
   ENDEE_PATH = "./endee_db"
   
   # Model configuration
   EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # SentenceTransformers model
   LLM_PROVIDER = "groq"  # or "ollama" for local
   LLM_MODEL = "llama-3.1-70b-versatile"  # or "llama3.1" for Ollama
   ```

4. **Environment Configuration**

   ```python
   # config.py
   import os
   import streamlit as st
   
   class Config:
       # LLM Configuration
       LLM_PROVIDER = st.secrets.get("LLM_PROVIDER", "groq")  # "groq" or "ollama"
       GROQ_API_KEY = st.secrets.get("GROQ_API_KEY", "")
       OLLAMA_BASE_URL = st.secrets.get("OLLAMA_BASE_URL", "http://localhost:11434")
       LLM_MODEL = st.secrets.get("LLM_MODEL", "llama-3.1-70b-versatile")
       
       # Embedding Configuration
       EMBEDDING_MODEL = st.secrets.get("EMBEDDING_MODEL", "all-MiniLM-L6-v2")
       
       # Database
       DATABASE_URL = st.secrets.get("DATABASE_URL", "")
       ENDEE_PATH = st.secrets.get("ENDEE_PATH", "./endee_db")
       
       # Other
       SECRET_KEY = st.secrets.get("SECRET_KEY", "")
       MAX_UPLOAD_SIZE = 200 * 1024 * 1024  # 200MB
   ```

5. **Deployment Steps**

   1. Connect GitHub repository to Streamlit Cloud
   2. Select main branch and `streamlit_app.py` as entry point
   3. Configure secrets in Streamlit Cloud dashboard
   4. Deploy application
   5. Access via `https://your-app-name.streamlit.app`

### Continuous Deployment

- **Automatic deployment** on git push to main branch
- **Branch-based deployments** for testing
- **Rollback capability** via Streamlit Cloud dashboard

### Local Development

```bash
# Setup virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download embedding model (happens automatically on first run)
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"

# Option 1: Setup Groq (Recommended)
# 1. Get API key from https://console.groq.com
# 2. Add to .env file

# Option 2: Setup Ollama (Local)
# 1. Install from https://ollama.ai
# 2. Run: ollama pull llama3.1
# 3. Start server: ollama serve

# Create local .env file
cp .env.example .env
# Edit .env with your configuration

# Run locally
streamlit run streamlit_app.py
```

**.env.example:**
```bash
# LLM Configuration (choose one)
# Option 1: Groq (recommended)
LLM_PROVIDER=groq
GROQ_API_KEY=gsk_your_key_here
LLM_MODEL=llama-3.1-8b-instant

# Option 2: Ollama (local)
# LLM_PROVIDER=ollama
# OLLAMA_BASE_URL=http://localhost:11434
# LLM_MODEL=llama3.1

# Embedding Model
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Database
DATABASE_URL=sqlite:///./data/app.db
SECRET_KEY=your-secret-key-change-this

# Endee
ENDEE_PATH=./endee_db
```

### Docker Setup (Optional Local Development)

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8501

CMD ["streamlit", "run", "streamlit_app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  streamlit:
    build: .
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - ./endee_db:/app/endee_db
    environment:
      # LLM Configuration
      - LLM_PROVIDER=${LLM_PROVIDER:-groq}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - LLM_MODEL=${LLM_MODEL:-llama-3.1-8b-instant}
      # Embedding
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-all-MiniLM-L6-v2}
      # Database
      - DATABASE_URL=sqlite:///./data/app.db
      - SECRET_KEY=${SECRET_KEY}
      - ENDEE_PATH=/app/endee_db
```

### Deployment Checklist

- [ ] GitHub repository configured
- [ ] requirements.txt includes all dependencies
- [ ] Streamlit Cloud account created
- [ ] Secrets configured in Streamlit Cloud
- [ ] Database initialization script tested
- [ ] API rate limits configured
- [ ] Error logging enabled
- [ ] Performance monitoring active
- [ ] User authentication implemented
- [ ] Data backup strategy defined

---

## Performance Metrics & Monitoring

### Metrics Collection Architecture

**Three-tier Metrics System:**

1. **Application Metrics** (in-memory)
2. **Persistent Metrics** (SQLite)
3. **Visualization Layer** (Streamlit/Plotly)

### Core Metrics Tracked

#### 1. System Performance Metrics

**Query Latency:**
- Mean response time
- P50, P95, P99 percentiles
- Maximum response time
- Latency distribution histogram

**Throughput:**
- Queries per second (QPS)
- Documents processed per hour
- Concurrent users
- Peak load handling

**Resource Usage:**
- Memory consumption
- CPU utilization
- Storage usage (Endee + SQLite)
- API quota consumption

**Code Implementation:**

```python
@dataclass
class PerformanceMetrics:
    query_latency: List[float]
    throughput_qps: float
    memory_usage_mb: float
    cpu_percent: float
    
    def get_percentile(self, p: int) -> float:
        return np.percentile(self.query_latency, p)
    
    def plot_latency_distribution(self):
        fig = px.histogram(
            self.query_latency,
            nbins=50,
            title="Query Latency Distribution"
        )
        fig.add_vline(
            x=self.get_percentile(95),
            line_dash="dash",
            annotation_text="P95"
        )
        return fig
```

#### 2. RAG Quality Metrics

**Retrieval Accuracy:**
- Precision@K (K=1,3,5,10)
- Recall@K
- Mean Reciprocal Rank (MRR)
- Normalized Discounted Cumulative Gain (NDCG)

**Answer Quality:**
- Answer relevance score (0-1)
- Citation accuracy rate
- Hallucination detection rate
- User satisfaction ratings

**Embedding Quality:**
- Embedding generation time
- Similarity score distribution
- Clustering quality metrics

**Visualization:**

```python
def plot_retrieval_metrics():
    k_values = [1, 3, 5, 10]
    precision_scores = [0.92, 0.88, 0.85, 0.79]
    recall_scores = [0.45, 0.67, 0.78, 0.88]
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=k_values, y=precision_scores,
        name="Precision@K", mode='lines+markers'
    ))
    fig.add_trace(go.Scatter(
        x=k_values, y=recall_scores,
        name="Recall@K", mode='lines+markers'
    ))
    fig.update_layout(
        title="Retrieval Performance",
        xaxis_title="K",
        yaxis_title="Score"
    )
    return fig
```

#### 3. Agent Performance Metrics

**Task Execution:**
- Task completion rate
- Average steps per task
- Planning accuracy
- Tool usage distribution

**Reasoning Quality:**
- Logic coherence score
- Step necessity ratio (useful steps / total steps)
- Backtracking frequency
- Dead-end detection

**Efficiency:**
- Time per agent step
- Tokens used per task
- Cost per task completion

#### 4. User Engagement Metrics

**Activity Metrics:**
- Daily/weekly/monthly active users
- Average session duration
- Queries per session
- Feature usage distribution

**Satisfaction Metrics:**
- User feedback ratings
- Task success rate
- Return user rate
- Feature adoption rate

### Metrics Dashboard Implementation

**Dashboard Layout:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SYSTEM OVERVIEW                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Avg      â”‚ â”‚ Active   â”‚ â”‚ Total    â”‚            â”‚
â”‚  â”‚ Latency  â”‚ â”‚ Users    â”‚ â”‚ Queries  â”‚            â”‚
â”‚  â”‚ 1.2s     â”‚ â”‚ 45       â”‚ â”‚ 12.5K    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PERFORMANCE TRENDS (24h)                           â”‚
â”‚  [Latency Chart]          [Throughput Chart]        â”‚
â”‚                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  RETRIEVAL ACCURACY                                 â”‚
â”‚  [Precision/Recall Curves]  [Score Distribution]    â”‚
â”‚                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  AGENT ANALYTICS                                    â”‚
â”‚  [Task Success Rate]     [Steps Distribution]       â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Code Example:**

```python
def render_metrics_dashboard():
    st.title("Performance Metrics Dashboard")
    
    # Key metrics row
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Avg Latency", "1.23s", delta="-0.15s")
    with col2:
        st.metric("Active Users", "45", delta="+5")
    with col3:
        st.metric("Total Queries", "12.5K", delta="+2.3K")
    with col4:
        st.metric("Success Rate", "94.2%", delta="+1.2%")
    
    # Performance trends
    st.subheader("Performance Trends (Last 24 Hours)")
    col1, col2 = st.columns(2)
    
    with col1:
        fig_latency = plot_latency_trends()
        st.plotly_chart(fig_latency, use_container_width=True)
    
    with col2:
        fig_throughput = plot_throughput_trends()
        st.plotly_chart(fig_throughput, use_container_width=True)
    
    # Retrieval metrics
    st.subheader("Retrieval Accuracy")
    fig_retrieval = plot_retrieval_metrics()
    st.plotly_chart(fig_retrieval, use_container_width=True)
    
    # Agent metrics
    st.subheader("Agent Performance")
    col1, col2 = st.columns(2)
    
    with col1:
        fig_success = plot_agent_success_rate()
        st.plotly_chart(fig_success, use_container_width=True)
    
    with col2:
        fig_steps = plot_steps_distribution()
        st.plotly_chart(fig_steps, use_container_width=True)
```

### Benchmarking Suite

**Performance Benchmarks:**

1. **Document Ingestion Speed**
   - Files per second
   - Chunks per second
   - Embeddings per second

2. **Query Performance**
   - Cold start latency
   - Warm cache latency
   - Concurrent query handling

3. **Vector Search Speed**
   - Search latency vs collection size
   - Accuracy vs speed tradeoff
   - Index rebuild time

4. **Agent Execution**
   - Simple task completion time
   - Complex task completion time
   - Multi-step efficiency

**Benchmark Results Storage:**

```python
@dataclass
class BenchmarkResult:
    benchmark_name: str
    timestamp: datetime
    metric_value: float
    metadata: Dict[str, Any]
    
    def save_to_db(self, conn):
        # Store in benchmarks table
        pass
    
    def compare_with_baseline(self, baseline):
        # Return percentage improvement
        pass
```

### Monitoring & Alerting

**Real-time Monitoring:**
- Error rate tracking
- Performance degradation detection
- API quota warnings
- Storage limit alerts

**Logging Strategy:**

```python
import logging

logger = logging.getLogger(__name__)

# Different log levels for different events
logger.info("Query processed successfully")
logger.warning("High latency detected: 5.2s")
logger.error("Vector search failed")
logger.critical("Database connection lost")
```

---

## Project Structure

```
multi-tenant-agentic-rag/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                    # FastAPI application entry
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py                # Authentication endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ documents.py           # Document management
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py                # Chat/query endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ agent.py               # Agent task endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ memory.py              # Memory endpoints
â”‚   â”‚   â”‚   â””â”€â”€ metrics.py             # Metrics endpoints
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base_agent.py          # Base agent class
â”‚   â”‚   â”‚   â”œâ”€â”€ planner.py             # Task planning logic
â”‚   â”‚   â”‚   â”œâ”€â”€ executor.py            # Step execution
â”‚   â”‚   â”‚   â”œâ”€â”€ tools.py               # Agent tools
â”‚   â”‚   â”‚   â””â”€â”€ memory_manager.py      # Agent memory
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py           # Vector retrieval
â”‚   â”‚   â”‚   â”œâ”€â”€ chunker.py             # Document chunking
â”‚   â”‚   â”‚   â”œâ”€â”€ embedder.py            # Embedding generation
â”‚   â”‚   â”‚   â”œâ”€â”€ reranker.py            # Result reranking
â”‚   â”‚   â”‚   â””â”€â”€ generator.py           # Answer generation
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ short_term.py          # Conversation memory
â”‚   â”‚   â”‚   â”œâ”€â”€ long_term.py           # Persistent memory
â”‚   â”‚   â”‚   â””â”€â”€ consolidator.py        # Memory consolidation
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ endee_client/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ client.py              # Endee wrapper
â”‚   â”‚   â”‚   â”œâ”€â”€ collections.py         # Collection management
â”‚   â”‚   â”‚   â””â”€â”€ multi_tenant.py        # Multi-tenancy helpers
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ users.py               # User management
â”‚   â”‚   â”‚   â”œâ”€â”€ sessions.py            # Session handling
â”‚   â”‚   â”‚   â””â”€â”€ security.py            # Password hashing, JWT
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ user.py                # User models
â”‚   â”‚   â”‚   â”œâ”€â”€ document.py            # Document models
â”‚   â”‚   â”‚   â”œâ”€â”€ conversation.py        # Chat models
â”‚   â”‚   â”‚   â””â”€â”€ metrics.py             # Metrics models
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ connection.py          # DB connection
â”‚   â”‚   â”‚   â”œâ”€â”€ schema.py              # Database schema
â”‚   â”‚   â”‚   â””â”€â”€ migrations/            # Schema migrations
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ logger.py              # Logging configuration
â”‚   â”‚   â”‚   â”œâ”€â”€ parsers.py             # File parsers
â”‚   â”‚   â”‚   â”œâ”€â”€ validators.py          # Input validation
â”‚   â”‚   â”‚   â””â”€â”€ helpers.py             # Utility functions
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ config.py                  # Application configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_rag.py
â”‚   â”‚   â”œâ”€â”€ test_agent.py
â”‚   â”‚   â”œâ”€â”€ test_endee.py
â”‚   â”‚   â””â”€â”€ test_api.py
â”‚   â”‚
â”‚   â”œâ”€â”€ requirements.txt               # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile                     # Docker configuration
â”‚   â””â”€â”€ .env.example                   # Environment template
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ streamlit_app.py               # Main Streamlit entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ 1_ğŸ“Š_Dashboard.py          # Home dashboard
â”‚   â”‚   â”œâ”€â”€ 2_ğŸ“„_Documents.py          # Document manager
â”‚   â”‚   â”œâ”€â”€ 3_ğŸ’¬_Chat.py               # Chat interface
â”‚   â”‚   â”œâ”€â”€ 4_ğŸ¤–_Agent.py              # Agent workspace
â”‚   â”‚   â”œâ”€â”€ 5_ğŸ“ˆ_Metrics.py            # Metrics dashboard
â”‚   â”‚   â””â”€â”€ 6_âš™ï¸_Settings.py          # Settings page
â”‚   â”‚
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chat_widget.py             # Chat UI component
â”‚   â”‚   â”œâ”€â”€ document_uploader.py       # Upload component
â”‚   â”‚   â”œâ”€â”€ metrics_charts.py          # Chart components
â”‚   â”‚   â”œâ”€â”€ agent_visualizer.py        # Agent step display
â”‚   â”‚   â””â”€â”€ citation_display.py        # Source citation UI
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ api_client.py              # Backend API client
â”‚   â”‚   â”œâ”€â”€ session_state.py           # State management
â”‚   â”‚   â””â”€â”€ formatters.py              # Display formatters
â”‚   â”‚
â”‚   â””â”€â”€ .streamlit/
â”‚       â”œâ”€â”€ config.toml                # Streamlit configuration
â”‚       â””â”€â”€ secrets.toml.example       # Secrets template
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ 01_embedding_tests.ipynb       # Embedding model comparison
â”‚   â”œâ”€â”€ 02_chunking_analysis.ipynb     # Chunking strategy tests
â”‚   â”œâ”€â”€ 03_retrieval_benchmark.ipynb   # Retrieval performance
â”‚   â”œâ”€â”€ 04_agent_evaluation.ipynb      # Agent performance
â”‚   â””â”€â”€ 05_endee_benchmarks.ipynb      # Endee performance tests
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md                # This file
â”‚   â”œâ”€â”€ API_DOCUMENTATION.md           # API reference
â”‚   â”œâ”€â”€ DEPLOYMENT_GUIDE.md            # Deployment instructions
â”‚   â”œâ”€â”€ ENDEE_INTEGRATION.md           # Endee usage guide
â”‚   â”œâ”€â”€ PERFORMANCE_BENCHMARKS.md      # Benchmark results
â”‚   â””â”€â”€ DESIGN_DECISIONS.md            # Architecture decisions
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample_documents/              # Test documents
â”‚   â””â”€â”€ benchmarks/                    # Benchmark datasets
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_db.py                     # Database initialization
â”‚   â”œâ”€â”€ seed_data.py                   # Sample data loader
â”‚   â”œâ”€â”€ run_benchmarks.py              # Benchmark suite
â”‚   â””â”€â”€ deploy.sh                      # Deployment script
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ tests.yml                  # CI/CD tests
â”‚       â””â”€â”€ deploy.yml                 # Auto-deployment
â”‚
â”œâ”€â”€ README.md                          # Project overview
â”œâ”€â”€ CONTRIBUTING.md                    # Contribution guidelines
â”œâ”€â”€ LICENSE                            # Project license
â”œâ”€â”€ .gitignore                         # Git ignore rules
â”œâ”€â”€ .env.example                       # Environment variables template
â”œâ”€â”€ docker-compose.yml                 # Docker compose config
â””â”€â”€ setup.py                           # Package setup
```

### File Organization Principles

1. **Separation of Concerns**
   - Backend logic separated from frontend
   - Clear module boundaries
   - Single responsibility per file

2. **Scalability**
   - Modular architecture
   - Easy to add new features
   - Plugin-style agent tools

3. **Testing**
   - Test files mirror source structure
   - Integration tests separate from unit tests
   - Benchmark scripts isolated

4. **Documentation**
   - Comprehensive docs folder
   - Inline code documentation
   - API documentation generated from code

---

## Implementation Timeline

### Project Duration: February 3 - February 11, 2026 (9 days)

### Phase-wise Work Distribution

---

### Phase 1: Foundation & Setup (Feb 3-4)

**Duration:** 2 days

#### Day 1: February 3, 2026 (Monday)

**Morning (9:00 AM - 1:00 PM): Project Initialization**

- Create GitHub repository with proper structure
- Initialize folder hierarchy as per project structure
- Set up virtual environment and install core dependencies
- Configure `.gitignore` and `.env.example`
- Create initial `README.md` with project overview
- Document problem statement and goals in README

**Afternoon (2:00 PM - 6:00 PM): Database & Core Backend**

- Implement SQLite database schema (`database/schema.py`)
- Create database initialization script (`scripts/init_db.py`)
- Implement Endee client wrapper (`endee_client/client.py`)
- Create multi-tenant collection management (`endee_client/multi_tenant.py`)
- Write basic authentication models (`auth/users.py`, `auth/security.py`)
- Test database connection and user creation

**Evening (7:00 PM - 9:00 PM): Documentation**

- Complete ARCHITECTURE.md (this document)
- Create API_DOCUMENTATION.md skeleton
- Document Endee integration approach in ENDEE_INTEGRATION.md

**Deliverables:**
- Project repository structure created
- Database schema implemented
- Endee client integrated
- Core authentication ready
- Documentation started

---

#### Day 2: February 4, 2026 (Tuesday)

**Morning (9:00 AM - 1:00 PM): Document Processing Pipeline**

- Implement file parsers (`utils/parsers.py`)
  - PDF parser (PyPDF2)
  - Text parser
  - DOCX parser (python-docx)
  - Markdown parser
- Create document chunking logic (`rag/chunker.py`)
  - Recursive character splitter
  - Chunk overlap configuration
  - Metadata extraction
- Test chunking with sample documents

**Afternoon (2:00 PM - 6:00 PM): Embedding & Storage**

- Implement embedding generator (`rag/embedder.py`)
  - SentenceTransformers integration (all-MiniLM-L6-v2)
    ```python
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(texts)
    ```
  - Batch processing
  - Caching mechanism
- Create document storage in Endee (`endee_client/collections.py`)
- Implement document upload API endpoint (`api/documents.py`)
- Test end-to-end document ingestion

**Evening (7:00 PM - 9:00 PM): Basic FastAPI Setup**

- Create FastAPI application (`main.py`)
- Implement CORS and middleware
- Add health check endpoint
- Configure logging (`utils/logger.py`)
- Test API locally

**Deliverables:**
- Document processing pipeline complete
- Embedding generation working
- Documents stored in Endee
- Basic API operational

---

### Phase 2: RAG Engine Development (Feb 5-6)

**Duration:** 2 days

#### Day 3: February 5, 2026 (Wednesday)

**Morning (9:00 AM - 1:00 PM): Vector Retrieval**

- Implement vector search (`rag/retriever.py`)
  - Query embedding generation
  - Similarity search in Endee
  - Top-K retrieval
  - Metadata filtering
- Create reranking logic (`rag/reranker.py`)
  - Cross-encoder reranking (optional)
  - Score normalization
- Test retrieval accuracy with sample queries

**Afternoon (2:00 PM - 6:00 PM): Answer Generation**

- Implement LLM integration (`rag/generator.py`)
  - Groq API integration (recommended - fast, free tier)
    ```python
    from groq import Groq
    client = Groq(api_key=config.GROQ_API_KEY)
    ```
  - OR Ollama integration (local, completely free)
    ```python
    import ollama
    response = ollama.chat(model='llama3.1', messages=[...])
    ```
  - Prompt engineering for RAG
  - Citation extraction
  - Answer formatting
- Create chat query endpoint (`api/chat.py`)
- Implement conversation history storage

**Evening (7:00 PM - 9:00 PM): RAG Quality Metrics**

- Implement retrieval metrics (`models/metrics.py`)
  - Precision@K calculation
  - Recall metrics
  - MRR computation
- Create basic benchmarking script (`scripts/run_benchmarks.py`)
- Run initial benchmark on test dataset

**Deliverables:**
- RAG pipeline fully functional
- Query endpoint working
- Basic metrics collection
- Benchmark results documented

---

#### Day 4: February 6, 2026 (Thursday)

**Morning (9:00 AM - 1:00 PM): Memory Management**

- Implement short-term memory (`memory/short_term.py`)
  - Conversation history tracking
  - Context window management
- Create long-term memory (`memory/long_term.py`)
  - Summary generation
  - Memory consolidation
- Implement memory retrieval in query pipeline

**Afternoon (2:00 PM - 6:00 PM): Performance Optimization**

- Add caching layer for embeddings
- Optimize database queries
- Implement async operations
- Add connection pooling
- Performance testing and profiling

**Evening (7:00 PM - 9:00 PM): Testing & Documentation**

- Write unit tests for RAG components (`tests/test_rag.py`)
- Create test dataset for evaluation
- Document RAG design decisions in DESIGN_DECISIONS.md
- Update README with RAG architecture

**Deliverables:**
- Memory system operational
- Performance optimized
- Unit tests passing
- Documentation updated

---

### Phase 3: Agentic Layer Implementation (Feb 7-8)

**Duration:** 2 days

#### Day 5: February 7, 2026 (Friday)

**Morning (9:00 AM - 1:00 PM): Agent Architecture**

- Implement base agent class (`agents/base_agent.py`)
  - Agent state management
  - Tool interface
  - Execution loop
- Create task planner (`agents/planner.py`)
  - Task decomposition
  - Step generation
  - Planning prompts

**Afternoon (2:00 PM - 6:00 PM): Agent Tools**

- Implement agent tools (`agents/tools.py`)
  - Vector search tool
  - Document retrieval tool
  - Summarization tool
  - Web search tool (optional)
- Create tool execution logic (`agents/executor.py`)
- Implement agent memory (`agents/memory_manager.py`)

**Evening (7:00 PM - 9:00 PM): Agent API Integration**

- Create agent task endpoint (`api/agent.py`)
- Implement task status tracking
- Add agent execution trace storage
- Test multi-step agent tasks

**Deliverables:**
- Agent framework implemented
- Tools functional
- Agent API endpoints ready
- Multi-step tasks working

---

#### Day 6: February 8, 2026 (Saturday)

**Morning (9:00 AM - 1:00 PM): Agent Refinement**

- Implement reasoning validation
- Add error handling and recovery
- Create agent performance metrics
  - Task completion rate
  - Steps efficiency
  - Tool usage stats
- Optimize agent prompts

**Afternoon (2:00 PM - 6:00 PM): Agent Testing**

- Write agent unit tests (`tests/test_agent.py`)
- Create complex test scenarios
- Run agent benchmarks
- Document agent performance

**Evening (7:00 PM - 9:00 PM): Integration Testing**

- End-to-end agent workflow testing
- Cross-component integration tests
- Performance profiling
- Bug fixes and optimization

**Deliverables:**
- Agent system refined
- Comprehensive testing complete
- Performance benchmarks documented
- Integration verified

---

### Phase 4: Streamlit UI Development (Feb 9)

**Duration:** 1 day

#### Day 7: February 9, 2026 (Sunday)

**Morning (9:00 AM - 1:00 PM): Core UI Components**

- Set up Streamlit app structure (`streamlit_app.py`)
- Create page navigation
- Implement document upload page (`pages/2_Documents.py`)
  - Drag-and-drop upload
  - Document list display
  - Delete functionality
- Create API client (`frontend/utils/api_client.py`)

**Afternoon (2:00 PM - 6:00 PM): Chat & Agent Interface**

- Implement chat interface (`pages/3_Chat.py`)
  - Message input/output
  - Streaming responses
  - Citation display
- Create agent workspace (`pages/4_Agent.py`)
  - Task submission form
  - Real-time step visualization
  - Agent trace display

**Evening (7:00 PM - 9:00 PM): Dashboard & Metrics**

- Create home dashboard (`pages/1_Dashboard.py`)
  - Key metrics display
  - Recent activity
  - Quick actions
- Implement metrics page (`pages/5_Metrics.py`)
  - Performance charts
  - Retrieval accuracy plots
  - Agent analytics
- Add settings page (`pages/6_Settings.py`)

**Deliverables:**
- Streamlit UI fully functional
- All pages implemented
- API integration complete
- User experience polished

---

### Phase 5: Metrics, Testing & Deployment (Feb 10-11)

**Duration:** 2 days

#### Day 8: February 10, 2026 (Monday)

**Morning (9:00 AM - 1:00 PM): Metrics Implementation**

- Create metrics collection system
  - Real-time metric tracking
  - Database storage
  - Aggregation functions
- Implement visualization components (`frontend/components/metrics_charts.py`)
  - Latency distribution plot
  - Precision/recall curves
  - Agent performance charts
  - Token usage graphs

**Afternoon (2:00 PM - 6:00 PM): Benchmarking**

- Run comprehensive benchmarks
  - Document ingestion speed
  - Query latency (various sizes)
  - Retrieval accuracy
  - Agent task completion
- Create benchmark notebooks in `experiments/`
  - `01_embedding_tests.ipynb`
  - `02_chunking_analysis.ipynb`
  - `03_retrieval_benchmark.ipynb`
  - `04_agent_evaluation.ipynb`
  - `05_endee_benchmarks.ipynb`

**Evening (7:00 PM - 9:00 PM): Results Documentation**

- Generate performance plots and save as images
- Create PERFORMANCE_BENCHMARKS.md
- Add benchmark results to README
- Document insights and findings

**Deliverables:**
- Metrics system operational
- Comprehensive benchmarks complete
- Performance plots generated
- Results documented

---

#### Day 9: February 11, 2026 (Tuesday) - Final Day

**Morning (9:00 AM - 12:00 PM): Deployment Preparation**

- Configure Streamlit Cloud deployment
  - Create `requirements.txt` final version
  - Set up `.streamlit/config.toml`
  - Prepare secrets configuration
- Create deployment documentation (DEPLOYMENT_GUIDE.md)
- Test local deployment with Docker
- Push code to GitHub repository

**Afternoon (12:00 PM - 4:00 PM): Final Testing & Polish**

- Run full integration test suite
- User acceptance testing
- Fix any critical bugs
- Performance optimization
- UI/UX refinements
- Code cleanup and formatting

**Evening (4:00 PM - 8:00 PM): Documentation & Submission**

- Finalize README.md
  - Add all required sections
  - Include performance plots (embedded as images)
  - Add comprehensive setup instructions
  - Document all features
- Complete all documentation files
  - ARCHITECTURE.md (final review)
  - API_DOCUMENTATION.md
  - DEPLOYMENT_GUIDE.md
  - DESIGN_DECISIONS.md
  - CONTRIBUTING.md
- Create demo video or GIF walkthrough
- Deploy to Streamlit Cloud
- Final repository cleanup
- Submit project

**Deliverables:**
- Application deployed to Streamlit Cloud
- All documentation complete
- README with plots and metrics
- Project ready for evaluation

---

### Work Distribution Summary

| Phase | Duration | Days | Key Focus |
|-------|----------|------|-----------|
| Phase 1: Foundation | 2 days | Feb 3-4 | Project setup, database, document processing |
| Phase 2: RAG Engine | 2 days | Feb 5-6 | Retrieval, generation, memory, optimization |
| Phase 3: Agentic Layer | 2 days | Feb 7-8 | Agent architecture, tools, testing |
| Phase 4: Streamlit UI | 1 day | Feb 9 | Complete frontend implementation |
| Phase 5: Metrics & Deploy | 2 days | Feb 10-11 | Benchmarking, deployment, documentation |

### Daily Time Commitment

- **Working hours per day:** 10-12 hours
- **Total project hours:** 90-108 hours
- **Weekday schedule:** 9 AM - 9 PM with breaks
- **Weekend schedule:** Flexible, 10-12 hours

### Risk Mitigation

**Potential Risks:**
1. Endee integration issues â†’ Allocate extra time on Day 1-2
2. Agent complexity â†’ Simplify initial implementation if needed
3. Deployment challenges â†’ Use Day 11 buffer time
4. Performance issues â†’ Optimize incrementally throughout

**Contingency Plans:**
- Buffer time built into Phase 5
- Modular design allows feature prioritization
- Core RAG functionality prioritized over advanced features
- Streamlit Cloud fallback: local demo if deployment issues

---

## Why This Architecture Wins

### 1. Endee is Central, Not Forced

Unlike typical vector database "demos" that bolt on a vector DB as an afterthought, this architecture:

- Uses Endee as the **core data primitive**
- All semantic operations flow through Endee
- Multi-tenancy built on Endee's collection model
- Agent reasoning stored in Endee for transparency

This demonstrates deep understanding of vector databases, not just surface-level usage.

### 2. RAG and Agents are Integrated, Not Separate

Most projects show either RAG **or** agents. This system:

- Seamlessly switches between RAG and agentic modes
- Agents use RAG as a tool
- Shared memory layer across both modes
- Unified retrieval infrastructure

This shows architectural sophistication beyond basic tutorials.

### 3. Multi-Tenant Design Mirrors Real SaaS Systems

This isn't a single-user toy project:

- Complete user isolation via namespacing
- Scalable architecture pattern
- Production-ready security considerations
- Real-world deployment model

This demonstrates understanding of production systems, not just prototypes.

### 4. UI Makes It Tangible

Many backend-heavy projects lack accessibility:

- Streamlit provides professional, interactive interface
- Real-time visualization of agent reasoning
- Metrics dashboards for transparency
- Deployment to Streamlit Cloud proves production-readiness

Users and evaluators can **interact** with the system, not just read code.

### 5. Benchmarks Turn You Into an Evaluator

Instead of claiming performance, this project **proves** it:

- Comprehensive benchmark suite
- Multiple performance dimensions measured
- Results visualized and documented
- Comparative analysis included

This positions you as someone who **evaluates** technology, not just uses it.

### 6. Production-Ready Engineering Practices

This project demonstrates professional software engineering:

- Proper project structure and organization
- Comprehensive testing strategy
- CI/CD integration
- Documentation-first approach
- Deployment automation
- Monitoring and observability

This is not a "student project" - it's a junior-to-mid level production system.

### 7. Metrics-Driven Development

Performance and quality are not afterthoughts:

- Metrics collection from day one
- Real-time performance monitoring
- Quality metrics for RAG and agent operations
- Visual dashboards for all stakeholders

This shows data-driven decision making and professional engineering practices.

### 8. Thoughtful Technology Choices

Every technology selection is justified:

- **Endee**: Lightweight, embeddable, perfect for multi-tenancy
- **Streamlit**: Rapid UI development, easy deployment
- **FastAPI**: Modern async API framework
- **SQLite**: Simple, reliable, sufficient for this scale
- **OpenAI**: Production-ready, reliable API

No over-engineering, no under-engineering - appropriate choices for the problem.

### 9. Clear Path from MVP to Scale

The architecture allows natural growth:

- Start: Single user, local deployment
- Grow: Multi-user, Streamlit Cloud
- Scale: Dedicated hosting, managed database, production LLM
- Enterprise: Kubernetes, multiple regions, advanced security

The foundation supports this progression without rewrites.

### 10. Complete Documentation

Every aspect is documented:

- Architecture decisions explained
- API fully documented
- Deployment process clear
- Performance characteristics measured
- Code extensively commented

This enables collaboration, maintenance, and evaluation.

---

## Conclusion

This architecture represents a **production-ready, multi-tenant, agentic RAG system** that:

- Solves real problems with elegant solutions
- Uses Endee as a core architectural component
- Demonstrates advanced AI/ML engineering
- Provides tangible, interactive results
- Measures and proves performance
- Deploys to production infrastructure
- Follows professional engineering practices

It is not a tutorial project - it is a **portfolio piece** that demonstrates readiness for production AI engineering roles.

---

**Document Version:** 1.0  
**Last Updated:** February 3, 2026  
**Authors:** Development Team  
**Status:** Final - Ready for Implementation
